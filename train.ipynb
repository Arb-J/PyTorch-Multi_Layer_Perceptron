{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48e3b43",
   "metadata": {},
   "source": [
    "# Creating our PyTorch training script\n",
    "\n",
    "With our neural network architecture implemented, we can move on to training the model using PyTorch.\n",
    "\n",
    "To accomplish this task, we'll need to implement a training script which:\n",
    "    \n",
    "    1) Creates an instance of our neural network architecture\n",
    "    2) Builds our dataset\n",
    "    3) Determines whether or not we are training our model on a GPU\n",
    "    4) Defines a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f23952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from torch.optim import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b71fd1",
   "metadata": {},
   "source": [
    "#### mlp: My definition of the multi-layer perceptrno architecture, implemented in PyTorch.\n",
    "\n",
    "#### SGD: The Stochastic Gradient Descent optimizer that we'll be using to train the model.\n",
    "\n",
    "#### make_blobs: Buildsa synthetic dataset of example data.\n",
    "\n",
    "#### train_test_split: Splits our dataset into a training and testing set.\n",
    "\n",
    "#### nn: PyTorch's neural network functionality.\n",
    "\n",
    "#### torch: The base PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc89279",
   "metadata": {},
   "source": [
    "When training a neural network, we do so in <i>batches</i> of data. Thefollowing function, <b>next_batch</b>, yields such batches to our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb28dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(inputs, targets, batchSize):\n",
    "    # loop over the dataset\n",
    "    for i in range(0, inputs.shape[0], batchSize):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (inputs[i:i + batchSize], targets[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907fd74",
   "metadata": {},
   "source": [
    "The <b>next_batch</b> function accepts three arguments:\n",
    "    \n",
    "    1) inputs: Our input data to the neural network.\n",
    "    2) targets: Our target output values (i.e., what we want our neural network to accurately predict).\n",
    "    3) batchSize:Size of data batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2299811",
   "metadata": {},
   "source": [
    "We have some important initializations to take care of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b006f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify our batch size, number of epochs and learning rate\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "LR = 1e-2\n",
    "\n",
    "# determine thedevice we will be using for training (CPU or GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[INFO] training using {}...\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f92205",
   "metadata": {},
   "source": [
    "Next, we need an example dataset to train our neural network on. Let's use scikit-learn's make_blobs function to create a synthetic dataset for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 3-class classification problem with 1000 data points\n",
    "# where each data point is a 4D feature vector\n",
    "print(\"[INFO] preparing data...\")\n",
    "(X, y) = make_blobs(n_samples=1000, n_features=4, centers=3, cluster_std=2.5, random_state=69)\n",
    "\n",
    "# create training and testing splits, and convert them to PyTorch tensors\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.15, random_state=60)\n",
    "trainX = torch.from_numpy(trainX).float()\n",
    "testX = torch.from_numpy(testX).float()\n",
    "trainY = torch.from_numpy(trainY).float()\n",
    "testY = torch.from_numpy(testY).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc27eed",
   "metadata": {},
   "source": [
    "Once our data is generated, we apply the train_test_split function to create our training split, 85% for training and 15% for evaluation.\n",
    "\n",
    "From there, the training and testing data is converted to PyTorch tensors from NumPy arrays, and then converted to the floating point data type.\n",
    "\n",
    "Let's now instantiate our PyTorch neural network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our model and display its architecture\n",
    "mlp = mlp.get_training_model().to(device)\n",
    "print(mlp)\n",
    "\n",
    "# initialize optimizer and loss function\n",
    "optimizer = SGD(mlp.parameters(), lr=LR)\n",
    "lossFunction = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ccdad5",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a template to summarize current training progress\n",
    "trainTemplate = \"Epoch: {} , Test loss: {:.3f} , Test accuracy: {:.3f}\"\n",
    "\n",
    "# loop through the epochs\n",
    "for epoch in range(0, epochs):\n",
    "    # initialize tracker variables and set our model to trainable\n",
    "    print(\"[INFO] Epoch: {}...\".format(epoch+1))\n",
    "    trainLoss = 0\n",
    "    trainAcc = 0\n",
    "    samples = 0\n",
    "    mlp.train()\n",
    "    \n",
    "    # loop over the current batch of data\n",
    "    for (batchX, batchY) in next_batch(trainX, trainY, batch_size):\n",
    "        # flash data to the current device, run it through our model\n",
    "        # and calculate loss\n",
    "        (batchX, batchY) = (batchX.to(device), batchY.to(device))\n",
    "        predictions = mlp(batchX)\n",
    "        loss = lossFunction(predictions, batchY.long())\n",
    "        \n",
    "        # zero the gradients accumulated from the previous steps,\n",
    "        # perform backpropagation and update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update training loss, accuracy and the number of samples visited\n",
    "        trainLoss += loss.item() * batchY.size(0)\n",
    "        trainAcc += (predictions.max(1)[1] == batchY).sum().item()\n",
    "        samples += batchY.size(0)\n",
    "        \n",
    "    # Display model progress on the current training batch\n",
    "    trainTemplate = \"Epoch: {} , Train loss: {:.3f} , Train accuracy: {:.3f}\"\n",
    "    print(trainTemplate.format(epoch + 1, (trainLoss / samples),\n",
    "                              (trainAcc/samples)))\n",
    "    \n",
    "    ###\n",
    "    # Initialize tracker variables for testing, then set our model to evaluation mode\n",
    "    testLoss = 0\n",
    "    testAcc = 0\n",
    "    samples = 0\n",
    "    mlp.eval()\n",
    "\n",
    "    # Initialize a no-gradient context\n",
    "    with torch.no_grad():\n",
    "        # Loop over the current batch of test data\n",
    "        for (batchX, batchY) in next_batch(testX, testY, batch_size):\n",
    "            # Flash the data to the current device\n",
    "            (batchX, batchY) = (batchX.to(device), batchY.to(device))\n",
    "        \n",
    "            # Run data through our model and calculate loss\n",
    "            predictions = mlp(batchX)\n",
    "            loss = lossFunction(predictions, batchY.long())\n",
    "        \n",
    "            # Update test loss, accuracy and the number of samples visited\n",
    "            testLoss += loss.item() * batchY.size(0)\n",
    "            testAcc += (predictions.max(1)[1] == batchY).sum().item()\n",
    "            samples += batchY.size(0)\n",
    "    \n",
    "        # Display model progress on the current test batch\n",
    "        testTemplate = \"epoch: {} test loss: {:.3f} test accuracy: {:.3f}\"\n",
    "        print(testTemplate.format(epoch + 1, (testLoss / samples),\n",
    "            (testAcc / samples)))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163fba0",
   "metadata": {},
   "source": [
    "We then loop over our number of desired training epochs. Inmediately inside this <b>for</b> loop we:\n",
    "    \n",
    "    1) Show the epoch number, which is useful for debugging purposes.\n",
    "    2) Initialize our training loss and accuracy\n",
    "    3) Initialize the total number of data points used inside the current iteration of the training loop\n",
    "    4) Put the PyTorch model in training mode\n",
    "    \n",
    "Then starts and inner <b>for</b> loop that loops over each of our batches in the training set. Nearly every training proedure you write using PyTorch will consist of an outer loop (over the number of epochs) and an inner loop (over the data batches).\n",
    "\n",
    "Within the inner loop (batch loop), we proceed to:\n",
    "    \n",
    "    1) Move the batchX and batchY data to our CPU or GPU.\n",
    "    2) Pass the batchX data through the neural network and make predictions on it.\n",
    "    3) Use our loss function to compute our loss by comparing the output <b>predictions</b> to our ground-truth class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7faec0",
   "metadata": {},
   "source": [
    "## At this point, we've trained our PyTorch model on all data points in an epoch - now we need to evaluate it on our testing set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
